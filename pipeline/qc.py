# pipeline/qc.py

from transformers import pipeline
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load sentence-transformer for semantic embedding
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

def evaluate_text_similarity(generated_text: str, original_prompt: str, summarizer) -> float:
    """
    Evaluate the quality of generated text by comparing summary to the original prompt.

    Adds 'summarize:' prefix for T5 models, which require explicit task tokens.

    Parameters:
    - generated_text (str): Text generated by LLM
    - original_prompt (str): Prompt used for generation
    - summarizer (pipeline): Summarization model from transformers

    Returns:
    - float: Cosine similarity score between summary and prompt (0 to 1)
    """

    # Add task prefix for T5 model if needed
    model_name = summarizer.model.name_or_path
    text_to_summarize = generated_text
    if "t5" in model_name.lower():
        text_to_summarize = "summarize: " + generated_text

    # Generate summary
    summary = summarizer(text_to_summarize, max_length=60, min_length=20, do_sample=False)[0]["summary_text"]

    # Compute semantic embeddings
    embeddings = embedding_model.encode([summary, original_prompt], convert_to_tensor=True)

    # Calculate cosine similarity
    similarity = cosine_similarity(
        embeddings[0].unsqueeze(0).numpy(),
        embeddings[1].unsqueeze(0).numpy()
    )[0][0]

    return float(similarity)

